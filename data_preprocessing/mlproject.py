# -*- coding: utf-8 -*-
"""MLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oHqRSISUB3H9T_eS98NhGzXrDYiXk5N_
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("fernandol/countries-of-the-world")

print("Path to dataset files:", path)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder,StandardScaler

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split,GridSearchCV

# Load Dataset
df=pd.read_csv(path+"/countries of the world.csv")

df.head()

df.info()

df.describe()

#Univariate Analysis
df['GDP ($ per capita)'].hist(bins=30, color='skyblue')
plt.title('Distribution of GDP per Capita')
plt.xlabel('GDP ($ per capita)')
plt.ylabel('Frequency')
plt.show()

sns.countplot(x='Region', data=df)
plt.title("Country Count by Region")

#Bivariate Analysis
sns.scatterplot(data=df, x='Literacy (%)', y='GDP ($ per capita)')
plt.title('GDP vs Literacy')
plt.show()

sns.scatterplot(x='Phones_(per_1000)', y='GDP_($_per_capita)', data=df)
plt.title("GDP vs Phones per 1000")
plt.show()

#Multivariate Analysis
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Clean Columns
df.columns = df.columns.str.strip().str.replace(" ", "_")

df.drop(["Country", "Other_(%)", "Infant_mortality_(per_1000_births)"], axis=1, inplace=True)

df.isnull().sum()

# Handle Numbers with Commas
for col in df.columns[1:]:
    df[col] = df[col].astype(str).str.replace(",", ".")
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Drop rows with NaNs
df.dropna(inplace=True)

numeric_cols = df.columns.drop("Region")
numeric_cols = numeric_cols.drop("GDP_($_per_capita)")
print(numeric_cols)

# Encode Categorical Column
le = LabelEncoder()
df['Region'] = le.fit_transform(df['Region'])

# Remove outliers using IQR
def remove_outliers_iqr(dataframe, columns):
    for col in columns:
        Q1 = dataframe[col].quantile(0.25)
        Q3 = dataframe[col].quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        dataframe = dataframe[(dataframe[col] >= lower) & (dataframe[col] <= upper)]
    return dataframe

df = remove_outliers_iqr(df, numeric_cols).reset_index(drop=True)

# Features and Target
X = df.drop("GDP_($_per_capita)", axis=1)
y = df["GDP_($_per_capita)"]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split Data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

#RandomForestRegressor
model1 = RandomForestRegressor(random_state=42)
model1.fit(X_train, y_train)

# Predict
y_pred_rf = model1.predict(X_test)

# Evaluation
print("R2 Score:", r2_score(y_test, y_pred_rf))
print("RMSE:", np.sqrt(mean_squared_error(y_test, y_pred_rf)))

from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.tree import plot_tree

#hyper parametric tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

rf = RandomForestRegressor(random_state=42)

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,
                           cv=5, scoring='r2', n_jobs=-1, verbose=1)

grid_search.fit(X_train, y_train)

# Best Estimator
best_rf = grid_search.best_estimator_
print("\nBest Parameters:", grid_search.best_params_)

# Predict
y_pred_rf = best_rf.predict(X_test)

# Evaluation
r2 = r2_score(y_test, y_pred_rf)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))
mae = mean_absolute_error(y_test, y_pred_rf)

print("\n--- Tuned Random Forest Evaluation ---")
print("RÂ² Score:", r2)
print("RMSE:", rmse)
print("MAE:", mae)

#decision tree
model2= DecisionTreeRegressor(random_state=42, max_depth=5)
model2.fit(X_train, y_train)

y_pred_dt= model2.predict(X_test)

mse_reg = mean_squared_error(y_test, y_pred_dt)
rmse_reg = np.sqrt(mse_reg)

r2_reg = r2_score(y_test, y_pred_dt)
print(rmse_reg)
print(r2_reg)

# 3. Train K-Nearest Neighbors Regressor Model

k_value_reg = 5
model3 = KNeighborsRegressor(n_neighbors=k_value_reg)
model3.fit(X_train, y_train)

y_pred_knn = model3.predict(X_test)

mse_reg = mean_squared_error(y_test, y_pred_knn)
rmse_reg = np.sqrt(mse_reg)
mae_reg = mean_absolute_error(y_test, y_pred_knn)
r2_reg = r2_score(y_test, y_pred_knn)

print(rmse_reg)
print(r2_reg)

#SVR
model_svr = SVR(kernel='rbf', C=1.0, epsilon=0.1, gamma='scale')
model_svr.fit(X_train, y_train)
y_pred_svr = model_svr.predict(X_test)

mse_reg = mean_squared_error(y_test, y_pred_svr)
rmse_reg = np.sqrt(mse_reg)
mae_reg = mean_absolute_error(y_test, y_pred_svr)
r2_reg = r2_score(y_test, y_pred_svr)

print(f"Regression Mean Squared Error (MSE): {mse_reg:.2f}")
print(f"Regression Root Mean Squared Error (RMSE): {rmse_reg:.2f}")
print(f"Regression Mean Absolute Error (MAE): {mae_reg:.2f}")
print(f"Regression R-squared (R2): {r2_reg:.2f}")
print("\n")

# Correlation Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title("Feature Correlation Heatmap")
plt.show()

# GDP Distribution
plt.figure(figsize=(8, 5))
sns.histplot(df["GDP_($_per_capita)"], kde=True, bins=30)
plt.title("Distribution of GDP per Capita")
plt.xlabel("GDP per Capita")
plt.show()

# GDP vs Literacy
plt.figure(figsize=(8, 5))
sns.scatterplot(x="Literacy_(%)", y="GDP_($_per_capita)", data=df)
plt.title("GDP vs Literacy Rate")
plt.xlabel("Literacy (%)")
plt.ylabel("GDP per Capita")
plt.show()

# Visualize
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred_rf)
plt.xlabel("Actual GDP per Capita")
plt.ylabel("Predicted GDP per Capita")
plt.title("Actual vs Predicted GDP per Capita")
plt.show()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.7, color='green')
plt.xlabel("Actual GDP")
plt.ylabel("Predicted GDP")
plt.title("Actual vs Predicted GDP (Random Forest)")
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.grid(True)
plt.show()

# Feature Importance Visualization
importances = best_rf.feature_importances_
feature_names = X.columns

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=feature_names)
plt.title("Feature Importances from Tuned Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Features")
plt.tight_layout()
plt.show()

# Visualize a single decision tree from the forest
plt.figure(figsize=(20, 10))
plot_tree(best_rf.estimators_[0],
          feature_names=feature_names,
          filled=True,
          rounded=True,
          max_depth=3,
          fontsize=10)
plt.title("Visualization of One Decision Tree from the Random Forest")
plt.show()

# Save model and scaler
import joblib
joblib.dump(best_rf, "gdp_model.pkl")

joblib.dump(scaler, "scaler.pkl")